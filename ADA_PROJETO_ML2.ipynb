{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WR6qRibeEBw"
      },
      "source": [
        "### Dataset: Brazilian E-Commerce Public Dataset by Olist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziE4ymmoeaNo"
      },
      "source": [
        "Conjunto de dados públicos de comércio eletrônico brasileiro por Olist\n",
        "\n",
        "Este é um conjunto de dados públicos de comércio eletrônico brasileiro de pedidos feitos na Olist Store. O conjunto de dados contém informações de 100 mil pedidos de 2016 a 2018 feitos em vários marketplaces no Brasil. Seus recursos permitem visualizar um pedido em múltiplas dimensões: desde status do pedido, preço, desempenho de pagamento e frete até localização do cliente, atributos do produto e finalmente avaliações escritas pelos clientes. Também um conjunto de dados de geolocalização que relaciona os CEPs brasileiros às coordenadas lat/lng.\n",
        "\n",
        "Estes são dados comerciais reais, foram anonimizados e as referências às empresas e parceiros no texto da revisão foram substituídas pelos nomes das grandes casas de Game of Thrones.\n",
        "\n",
        "Olist, a maior loja de departamentos dos mercados brasileiros. Olist conecta pequenas empresas de todo o Brasil a canais sem complicações e com um único contrato. Esses comerciantes podem vender seus produtos através da Olist Store e enviá-los diretamente aos clientes usando os parceiros logísticos da Olist. Veja mais em nosso site: www.olist.com\n",
        "\n",
        "Depois que um cliente compra o produto na Olist Store, um vendedor é notificado para atender ao pedido. Assim que o cliente recebe o produto, ou vence a data estimada de entrega, o cliente recebe por e-mail uma pesquisa de satisfação onde pode dar uma nota da experiência de compra e anotar alguns comentários.\n",
        "Atenção\n",
        "\n",
        "Um pedido pode ter vários itens.\n",
        "Cada item pode ser entregue por um vendedor distinto.\n",
        "Todos os textos de identificação de lojas e parceiros foram substituídos pelos nomes das grandes casas de Game of Thrones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFIGQMHffng"
      },
      "source": [
        "\n",
        "PNL:\n",
        "\n",
        "Este conjunto de dados oferece um ambiente supremo para analisar o texto das avaliações em suas múltiplas dimensões.\n",
        "\n",
        "Agrupamento:\n",
        "\n",
        "Alguns clientes não escreveram um comentário. Mas por que eles estão felizes ou bravos?\n",
        "\n",
        "Previsão de vendas:\n",
        "\n",
        "Com as informações da data de compra, você poderá prever vendas futuras.\n",
        "\n",
        "Desempenho de entrega:\n",
        "\n",
        "Você também poderá trabalhar o desempenho da entrega e encontrar maneiras de otimizar os prazos de entrega.\n",
        "\n",
        "Qualidade do produto:\n",
        "\n",
        "Divirta-se descobrindo as categorias de produtos mais propensas à insatisfação do cliente.\n",
        "\n",
        "Engenharia de recursos:\n",
        "\n",
        "Crie recursos a partir deste rico conjunto de dados ou anexe algumas informações públicas externas a ele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importando bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!python -m spacy download pt_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/chenyenpin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/chenyenpin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import (train_test_split as tst,\n",
        "                                     RandomizedSearchCV as rcv)   \n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "                               \n",
        "import os\n",
        "\n",
        "# NLP MODULES\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "import spacy\n",
        "import nltk\n",
        "from spacy.lang.pt.stop_words import STOP_WORDS as pt_stop\n",
        "\n",
        "\n",
        "from dotenv import (find_dotenv,\n",
        "                    load_dotenv)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "nlp=spacy.load(\"pt_core_news_md\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words=nltk.corpus.stopwords.words('portuguese')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "207"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a,à,ao,aos,aquela,aquelas,aquele,aqueles,aquilo,as,às,até,com,como,da,das,de,dela,delas,dele,deles,depois,do,dos,e,é,ela,elas,ele,eles,em,entre,era,eram,éramos,essa,essas,esse,esses,esta,está,estamos,estão,estar,estas,estava,estavam,estávamos,este,esteja,estejam,estejamos,estes,esteve,estive,estivemos,estiver,estivera,estiveram,estivéramos,estiverem,estivermos,estivesse,estivessem,estivéssemos,estou,eu,foi,fomos,for,fora,foram,fôramos,forem,formos,fosse,fossem,fôssemos,fui,há,haja,hajam,hajamos,hão,havemos,haver,hei,houve,houvemos,houver,houvera,houverá,houveram,houvéramos,houverão,houverei,houverem,houveremos,houveria,houveriam,houveríamos,houvermos,houvesse,houvessem,houvéssemos,isso,isto,já,lhe,lhes,mais,mas,me,mesmo,meu,meus,minha,minhas,muito,na,não,nas,nem,no,nos,nós,nossa,nossas,nosso,nossos,num,numa,o,os,ou,para,pela,pelas,pelo,pelos,por,qual,quando,que,quem,são,se,seja,sejam,sejamos,sem,ser,será,serão,serei,seremos,seria,seriam,seríamos,seu,seus,só,somos,sou,sua,suas,também,te,tem,tém,temos,tenha,tenham,tenhamos,tenho,terá,terão,terei,teremos,teria,teriam,teríamos,teu,teus,teve,tinha,tinham,tínhamos,tive,tivemos,tiver,tivera,tiveram,tivéramos,tiverem,tivermos,tivesse,tivessem,tivéssemos,tu,tua,tuas,um,uma,você,vocês,vos'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\",\".join(stop_words)\n",
        "not_stop=[\"era\",\"eram\",\"éramos\",\"esta\",\"está\",\"estamos\",\"estão\",\"estar\",\n",
        "          \"estava\",\"estavam\",\"estávamos\",\"esteja\",\"estejam\",\"estejamos\",\"esteve\",\"estive\",\"estivemos\",\n",
        "          \"estiver\",\"estivera\",\"estiveram\",\"estivéramos\",\"estiverem\",\"estivermos\",\"estivesse\",\"estivessem\",\n",
        "          \"estivéssemos\",\"estou\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funcoes necessarias para desenvolvimento do trabalho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get the datas\n",
        "def load_data(path:str, path1:str, path2:str)->pd.DataFrame:\n",
        "    '''\n",
        "    params:\n",
        "    path:string\n",
        "    return dataframe\n",
        "    '''\n",
        "    load_dotenv(find_dotenv())\n",
        "    path=os.getenv(\"path\")\n",
        "    path1=os.getenv(\"path1\") \n",
        "    path2=os.getenv(\"path2\")\n",
        "    df_rev=pd.read_csv(path)\n",
        "    def_cust=pd.read_csv(path1)\n",
        "    df_item=pd.read_csv(path2)\n",
        "    return df_rev, def_cust, df_item\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Eda Function\n",
        "def eda(df:pd.DataFrame)->None:\n",
        "  '''\n",
        "  Retornara um analise exploratoria simples\n",
        "\n",
        "  -param df: pandas dataframe\n",
        "  '''\n",
        "  print(\"-\"*30,\"DataFrame Shape\", \"-\"*30)\n",
        "  print(f\"Columns {df.shape[0]}\\nRows {df.shape[1]}\")\n",
        "  print(\"-\"*30,\"DataFrame info\", \"-\"*30)\n",
        "  print(df.info())\n",
        "  print(\"-\"*30,\"DataFrame Nan\", \"-\"*30)\n",
        "  print(df.isna().sum())\n",
        "  print(\"-\"*30,\"DataFrame duplicated\", \"-\"*30)\n",
        "  print(df.duplicated().sum())\n",
        "  print(\"-\"*30,\"DataFrame nunique\", \"-\"*30)\n",
        "  print(df.nunique())\n",
        "  \n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "#Lower casing the words\n",
        "\n",
        "def lower_case(column):\n",
        "   '''\n",
        "   Retorna as coluna selecionada para lower case\n",
        "   '''\n",
        "   return column.lower()\n",
        "\n",
        "#---------------------------------------------------------\n",
        "def remove_punctuation(column):\n",
        "    '''\n",
        "    Function to remove punctuation from the input string\n",
        "    '''\n",
        "    column = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", column)\n",
        "    return column\n",
        "\n",
        "#-------------------------------------------------------\n",
        "\n",
        "def tokenize(columns:str)->pd.Series:\n",
        "   tokenize= word_tokenize(columns)\n",
        "   column_withno_stopwords=[token for token in tokenize  if not token in stop_words ]\n",
        "   new_sentence=\" \".join(column_withno_stopwords)\n",
        "   return new_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_rev, df_cust, df_item=load_data(\"path\",\"path1\",\"path2\")\n",
        "df_item=df_item[['order_id',\n",
        "     'price', 'freight_value']]\n",
        "df_rev=df_rev[['order_id', 'review_score',\n",
        "       'review_comment_message', 'review_creation_date']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.merge(df_rev, df_item, how=\"inner\", on=\"order_id\").copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['review_comment_message'] = df['review_comment_message'].apply(lambda x :lower_case(x))\n",
        "df[\"review_comment_message\"]=df[\"review_comment_message\"].apply(lambda x :remove_punctuation(x))\n",
        "df[\"token\"]=df[\"review_comment_message\"].apply(lambda x: tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                     recebi bem antes do prazo estipulado\n",
              "1        parabéns lojas lannister adorei comprar pela i...\n",
              "2        aparelho eficiente no site a marca do aparelho...\n",
              "3                 mas um pouco travandopelo valor ta boa\\n\n",
              "4        vendedor confiável produto ok e entrega antes ...\n",
              "                               ...                        \n",
              "47637    para este produto recebi de acordo com a compr...\n",
              "47638    entregou dentro do prazo o produto chegou em c...\n",
              "47639    o produto não foi enviado com nf não existe ve...\n",
              "47640    excelente mochila entrega super rápida super r...\n",
              "47641    meu produto chegou e ja tenho que devolver poi...\n",
              "Name: review_comment_message, Length: 47642, dtype: object"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"review_comment_message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
